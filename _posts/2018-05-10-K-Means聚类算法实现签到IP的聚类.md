---
layout: post
title: K-Means聚类算法实现签到IP的聚类
categories:
  - K-Means
  - 机器学习
  - 算法

description: K-Means聚类算法实现签到IP的聚类
keywords: K-Means,聚类,算法,机器学习
comments: true
---


# K-Means聚类算法实现签到IP的聚类
> 适用系统:Ubuntu16.04, Deepin15.5, Mac OS

####　K-Means概念
* 聚类（Clustering）：K-Means 是一种聚类分析（Cluster Analysis）方法。聚类就是将数据对象分组成为多个类或者簇 (Cluster)，使得在同一个簇中的对象之间具有较高的相似度，而不同簇中的对象差别较大。
* 划分（Partitioning）：聚类可以基于划分，也可以基于分层。划分即将对象划分成不同的簇，而分层是将对象分等级。
* 排他（Exclusive）：对于一个数据对象，只能被划分到一个簇中。如果一个数据对象可以被划分到多个簇中，则称为可重叠的（Overlapping）。
* 距离（Distance）：基于距离的聚类是将距离近的相似的对象聚在一起。基于概率分布模型的聚类是在一组对象中，找到能符合特定分布模型的对象的集合，他们不一定是距离最近的或者最相似的，而是能完美的呈现出概率分布模型所描述的模型。

#### K-Means算法的实现
* 由于 K-Means 算法值针对给定的完整数据集进行操作，不需要任何特殊的训练数据，所以 K-Means 是一种无监督的机器学习方法（Unsupervised Machine Learning Technique）。
* KMeans算法试图使集群数据分为n组独立数据样本，使n组集群间的方差相等，数学描述为最小化惯性或集群内的平方和。该算法的缺点在于需要提前确定数据集群的类别n，虽然有些预处理算法可以确定n的取值，但是会增加很大一部分的计算复杂度。规模适合大量的样品，被使用在在许多不同的领域。

* 在介绍 k-means 的具体步骤之前，让我们先来看看它对于需要进行聚类的数据的一个基本假设吧：对于每一个 cluster ，我们可以选出一个中心点(center)，使得该cluster中的所有的点到该中心点的距离小于到其他 cluster 的中心的距离。虽然实际情况中得到的数据并不能保证总是满足这样的约束，但这通常已经是我们所能达到的最好的结果，而那些误差通常是固有存在的或者问题本身的不可分性造成的。例如下图所示的两个高斯分布，从两个分布中随机地抽取一些数据点出来，混杂到一起，现在要让你将这些混杂在一起的数据点按照它们被生成的那个分布分开来：
![K-Meansone](/images/posts/ML/kmeans01.png)
* 由于这两个分布本身有很大一部分重叠在一起了，例如，对于数据点2.5来说，它由两个分布产生的概率都是相等的，你所做的只能是一个猜测；稍微好一点的情况是2，通常我们会将它归类为左边的那个分布，因为概率大一些，然而此时它由右边的分布生成的概率仍然是比较大的，我们仍然有不小的几率会猜错。而整个阴影部分是我们所能达到的最小的猜错的概率，这来自于问题本身的不可分性，无法避免。因此，我们将 k-means 所依赖的这个假设看作是合理的。

#### 数学建模
* k-means算法将n组样本划分为不相交的集群Ck每个集群中用样本的均值μk所描述的。这些均值通常称为集群“重心”(加权即为重心，不加权就是中心)。请注意,一般来说,他们不是每个集群Ck中的点,尽管重心和Ck中的点在同一个空间。k - means算法旨在选择重心,减少惯性,或集群内平方和的准则:

![K-Meansone](/images/posts/ML/kmeans02.png)

* 这个函数，其中rnk在数据点 n被归类到 clusterk的时候为 1 ，否则为 0 。直接寻找 rnk 和μk来最小化 J 并不容易，不过我们可以采取迭代的办法：先固定 μk ，选择最优的rnk ，很容易看出，只要将数据点归类到离他最近的那个中心就能保证 J 最小。下一步则固定rnk，再求最优的 μk。将J 对μk 求导并令导数等于零，很容易得到 J最小的时候 μk

* 应该满足：
![K-Meansone](/images/posts/ML/kmeans03.png)

* 亦即 μk的值应当是所有 cluster k 中的数据点的平均值。由于每一次迭代都是取到 J 的最小值，因此 J 只会不断地减小（或者不变），而不会增加，这保证了 k-means 最终会到达一个极小值。虽然 k-means 并不能保证总是能得到全局最优解，但是对于这样的问题，像 k-means 这种复杂度的算法，这样的结果已经是很不错的了。

* 惯性或集群内平方和准则,可以被视为衡量内部是如何耦合的标准。它存在各种各样的缺点:
1. 惯性必须假设集群数据是凸和各向同性的,然而数据并非总是能保持这个特性。细长的集群,或不规则形状将响应差。
2. 惯性不是一个标准化的指标:我们只知道更低的值是更好的和零是最优的。但在高维空间,欧几里得距离往往会变得膨胀(这是所谓的“维数的诅咒”的一个实例)。在使用k-means聚类之前运用如PCA降维可以缓解这个问题,加速计算。 
![K-Meansone](/images/posts/ML/kmeans04.png)

* k-means是基于欧几里得距离公式来计算的，所以它是适用于任何高维空间的，当然这个思想其实可以扩展到任意可以用来评价相似性的基础公式，如切比雪夫距离和相关性公式等。


#### 签到数据进行聚类分析的实现
>接下来主要讲解如何使用K-Means算法对签到数据进行聚类分析,从而可以从中查找到异常的IP值

```
* 1 爬取数据,本篇文章所使用的数据是从公司内网爬取的,共14286条数据
* 2 对数据进行TF-IDF加权处理
* 3 创建聚类器
* 4 设置参数并对数据进行训练
* 5 保存模型返回分类索引
* 6 拼接原始数据
* 7 为数据添加分类标签并导出数据
```

##### 导入头文件
```
import csv
import time
import pandas as pd
from lxml import etree
from urllib import request
from urllib.parse import urlencode
from sklearn.cluster import KMeans
from sklearn.externals import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
```
##### 创建类并进行初始化
```
class SignKmeans(object):

    # 初始化
    def __init__(self, filename=None, n_cluster=5):

        if not filename: raise Exception('请输入文件名')
        self.sign_data = pd.read_csv(filename)
        self.n_cluster = n_cluster
```
> n_cluser为聚类器的聚类数量

##### TF-IDF数据加权处理

```
def __tfidf_deal(self):

    print('正在对数据进行加权与聚类处理...')
    sign_datas = self.sign_data['IP']

    tfidf = TfidfVectorizer()
    tfidf_datas = tfidf.fit_transform(sign_datas)

    print(tfidf.get_feature_names())
    print(tfidf_datas.toarray())

    return tfidf_datas
```

##### 使用K-Means算法对数据进行聚类
```
  def kmeans_cluster(self):

        # 创建聚类器
        n_cluster = 5
        km = KMeans(n_clusters=n_cluster,
                         max_iter=300,
                         n_init=50,
                         init='k-means++',
                         n_jobs=30)

        km.fit(self.__tfidf_deal())

        # 对数据进行聚类的索引结果为
        kmlabels = km.labels_.tolist()
        print('聚类完成')

        # 保存模型
        joblib.dump(km, '../Datas/Models/Signmodel.pkl')
        print('模型保存成功')

        return kmlabels
```
>n_clusters 聚类数量, 
>max_iter 最大迭代次数, 
>n_init 重新选择初始值的次数, 
>init 制定初始值选择算法, 
>n_jobs 最大进程数,为-1时CPU満载运行


